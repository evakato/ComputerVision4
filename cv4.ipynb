{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl8TpaC799+YwP0pwwQXva",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evakato/ComputerVision4/blob/main/cv4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import the Fashion MNIST dataset including the data labels. This would import two sets (training set and test set). Create a third set (validation set) by splitting the training set into two (training set and validation set) for validation purposes. Decide what a good ratio of training/validation is, and motivate your choice. You should use the validation set to evaluate the different choices you make when building your CNNs. Keep in mind that the test set will only be used at the very final stage and will not be included in the validation step."
      ],
      "metadata": {
        "id": "VIpBlt59Tmmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "val_images = val_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "print(\"Training images shape:\", train_images.shape)\n",
        "print(\"Training labels shape:\", train_labels.shape)\n",
        "print(\"Validation images shape:\", val_images.shape)\n",
        "print(\"Validation labels shape:\", val_labels.shape)\n",
        "print(\"Test images shape:\", test_images.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhn0qdxVSyW0",
        "outputId": "27fd0d63-e3b0-468b-d6d1-40f0ece51573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images shape: (48000, 28, 28)\n",
            "Training labels shape: (48000,)\n",
            "Validation images shape: (12000, 28, 28)\n",
            "Validation labels shape: (12000,)\n",
            "Test images shape: (10000, 28, 28)\n",
            "Test labels shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Recreate the LeNet-5 architecture as your baseline. The model takes as input a greyscale image of size 28x28x1 and has 10 outputs, one for each class. Make sure all parameters (number of neurons, number and size of kernels) is the same as in the original architecture. You may assume that no zero-padding was applied. The model is trained using cross-entropy loss, Adam optimizer with a learning rate of 0.001. Use torch.nn.init.kaiming_uniform to initialize your weights. Use a batch size of 32, unless your hardware doesn't allow you to. Then reduce the size accordingly.\n"
      ],
      "metadata": {
        "id": "OFGPmPXTT5i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)  # 1 input channel, 6 output channels, 5x5 kernel\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 6 input channels, 16 output channels, 5x5 kernel\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 16 channels, 4x4 feature map size\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)  # 10 output classes\n",
        "\n",
        "        # Weight initialization\n",
        "        kaiming_uniform_(self.conv1.weight)\n",
        "        kaiming_uniform_(self.conv2.weight)\n",
        "        kaiming_uniform_(self.fc1.weight)\n",
        "        kaiming_uniform_(self.fc2.weight)\n",
        "        kaiming_uniform_(self.fc3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with ReLU activation and max pooling\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 2) # Max pooling with 2x2 window\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        # Flatten before fully connected layers\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        # Fully connected layers with ReLU activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # All dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "model = LeNet5()\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch_size = 32\n",
        "print(\"Batch size:\", batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BnWpAn0T9XS",
        "outputId": "10a5e100-58b6-4c30-eceb-94a405d5f53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeNet5(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "Batch size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Now create four model variants. Each model may differ from the previous model by only one aspect, such that we can compare each pair of subsequent models pair-wise. An aspect should be a meaningful property, e.g., change the type of one layer (convolution --> pooling, etc.), add one layer, use dropout, change your activation function, change the number or size of your kernels, change the learning rate, etc. No use of any merging, attention, recurrent or locally-connected layers. Your variants should be aimed at getting a better performance. We keep the batch sizes fixed so choose a number and keep it constant for all models (including the baseline model)."
      ],
      "metadata": {
        "id": "K7m3tcJzWwOR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-05fnwZKWx_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}