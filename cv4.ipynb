{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evakato/ComputerVision4/blob/main/cv4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = (\n",
        "    #Nvidia GPU\n",
        "    \"cuda\" \n",
        "    if torch.cuda.is_available()\n",
        "    #Apple GPU\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    #Other\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIpBlt59Tmmv"
      },
      "source": [
        "1. Import the Fashion MNIST dataset including the data labels. This would import two sets (training set and test set). Create a third set (validation set) by splitting the training set into two (training set and validation set) for validation purposes. Decide what a good ratio of training/validation is, and motivate your choice. You should use the validation set to evaluate the different choices you make when building your CNNs. Keep in mind that the test set will only be used at the very final stage and will not be included in the validation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhn0qdxVSyW0",
        "outputId": "27fd0d63-e3b0-468b-d6d1-40f0ece51573"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#NOTE: data is not normalised yet\n",
        "train_data = datasets.FashionMNIST('Data', download=True, train=True, transform=ToTensor())\n",
        "test_data = datasets.FashionMNIST('Data', download=True, train=False, transform=ToTensor())\n",
        "\n",
        "splitlength = [50000,10000] \n",
        "#NOTE: This needs to be changed to be calculated using a percentage if we are going to have variable data lenghts (due to e.g. data augmentation)\n",
        "train_data, val_data = random_split(train_data, splitlength)\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28]) 4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x2e786844750>"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAh6klEQVR4nO3da3CU5f3G8WtzWgIkG5KQEwQaUMBySCuFmEEolgwhtY4obUV9AdaBkQamSK1OOipqO03/2LGOluKbCrUjeOgIKKNUBBNEAxaQZrDKkBgNSBIEm2wSct7n/4IxbeTkfZPkTsL3M7MzZHevPHeePOHKk939rc/zPE8AAPSyMNcLAABcmSggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5EuF7A14VCIZ04cUIxMTHy+XyulwMAMOR5nurr65WWlqawsAuf5/S5Ajpx4oTS09NdLwMAcJmOHTumkSNHXvD2PldAMTExrpeAK9jMmTONMw0NDcaZpqYm48yYMWOMMx9//LFxRpI++eQTqxzwvy71/3mPFdDatWv1+OOPq7q6WpmZmXr66ac1ffr0S+b4sxtciogw/5EIDw/vlUxkZKRx5mJ//sCl2fx/xHjN/7rU/uuRo/PFF1/UqlWrtHr1ah08eFCZmZnKzc3VyZMne2JzAIB+qEcK6IknntCSJUt011136dvf/raeeeYZDR48WM8++2xPbA4A0A91ewG1trbqwIEDysnJ+e9GwsKUk5OjkpKSc+7f0tKiYDDY5QIAGPi6vYBOnTqljo4OJScnd7k+OTlZ1dXV59y/sLBQgUCg88Iz4ADgyuD8EcqCggLV1dV1Xo4dO+Z6SQCAXtDtz4JLTExUeHi4ampqulxfU1OjlJSUc+7v9/vl9/u7exkAgD6u28+AoqKiNHXqVO3cubPzulAopJ07dyo7O7u7NwcA6Kd65HVAq1at0qJFi/S9731P06dP15NPPqnGxkbdddddPbE5AEA/1CMFdNttt+mLL77Qww8/rOrqan3nO9/R9u3bz3liAgDgyuXz+tjLdoPBoAKBgOtlXFFsp0/0sUOnC9tnU1ZWVhpn6urqjDM2LzdITEw0zrzxxhvGGUlasGCBVa439Oa0lL58jPcHdXV1io2NveDtzp8FBwC4MlFAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADAiR6Zho3+pa8PXBw2bJhxZtu2bVbb+u53v2uc2bVrl3EmNTXVOPPpp58aZ1577TXjjCQ9++yzxpmf/exnVtsy1dePV3xznAEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACaZh92E+n884YzMpOCLC7jCYMGGCcWbw4MHGmffff984Exsba5yRpNtvv904M3PmTOPM4cOHjTO/+93vjDPTp083zkhSa2urVc7UxIkTjTNNTU3GmU8++cQ4g57HGRAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOMEw0l4SFmbe9aFQyDiTlJRknLEdWGkz4NFmsOjTTz9tnElISDDOSNLChQuNM83NzcaZDRs2GGemTZtmnLn22muNM5LdkNCysjLjzB/+8AfjjM3abPfDwYMHrXL4ZjgDAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnfJ7nea4X8b+CwaACgYDrZfRb119/vXGmoaHBaluHDh0yzqxYscI489BDDxln9uzZY5yRpIgI8/m8mZmZxpnXX3/dOPP973/fOHPixAnjjCS1tbUZZ2bMmGGcWbZsmXHm+eefN86MHTvWOCNJjY2Nxpnq6mqrbQ1EdXV1io2NveDtnAEBAJyggAAATnR7AT3yyCPy+XxdLhMmTOjuzQAA+rkeeUO6iRMn6q233vrvRiz+rg4AGNh6pBkiIiKUkpLSE58aADBA9MhjQEePHlVaWprGjBmjO++8U5WVlRe8b0tLi4LBYJcLAGDg6/YCysrK0oYNG7R9+3atW7dOFRUVmjlzpurr6897/8LCQgUCgc5Lenp6dy8JANAHdXsB5eXl6Sc/+YmmTJmi3Nxcvf7666qtrdVLL7103vsXFBSorq6u83Ls2LHuXhIAoA/q8WcHxMXFady4cSorKzvv7X6/X36/v6eXAQDoY3r8dUANDQ0qLy9XampqT28KANCPdHsB3XfffSouLtann36q9957T7fccovCw8N1++23d/emAAD9WLf/Ce748eO6/fbbdfr0aQ0fPlzXX3+99u7dq+HDh3f3pgAA/RjDSPswmz9b2gxd3Ldvn3FGkqZPn26cefXVV40ze/fuNc5ERkYaZyQpFAoZZ3w+n3EmLi7OOGMzGNNmqKgktba2GmcGDx5snLE5hiZOnGic+fzzz40zkt2g2YMHD1ptayBiGCkAoE+igAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMMI+3DbIYuhoeHG2dKS0uNM5L0zjvvGGcGDRpknKmurjbO2L7JYUdHh3EmIqLH39fRmu0wUpvjyGYoa3R0tHGmqanJODN37lzjjCSNGzfOOFNVVWWcqa+vN870BwwjBQD0SRQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADjRd8f4wmpS8Icffmicueuuu4wzkt3E6RMnThhnbKZN20y1liSb4fA2k6NbWlqMM+3t7cYZWzbbioyMNM40NjYaZ0aNGmWcWbFihXFGkjZt2mScSUpKMs4M1GnYl8IZEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4wTDSPsxmyGVTU5NxZvbs2cYZSRo+fLhxJhgMGmfa2tqMM6FQyDhjy2af2wyabW1tNc7YDFe13VZUVJRxxufzGWdsBphed911xhlJ2rFjh3HGdp9fiTgDAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnGEbah8XGxvbKdt5++22r3I033micGTJkiNW2TIWF2f1uZTOE02YY6aBBg4wzNgNMbQZ3StKZM2eMMzExMcaZlpYW40xcXJxx5u9//7txRpI+++wz48yIESOstnUl4gwIAOAEBQQAcMK4gHbv3q2bbrpJaWlp8vl82rJlS5fbPc/Tww8/rNTUVEVHRysnJ0dHjx7trvUCAAYI4wJqbGxUZmam1q5de97b16xZo6eeekrPPPOM9u3bpyFDhig3N1fNzc2XvVgAwMBh/CSEvLw85eXlnfc2z/P05JNP6sEHH9TNN98sSXruueeUnJysLVu2aOHChZe3WgDAgNGtjwFVVFSourpaOTk5ndcFAgFlZWWppKTkvJmWlhYFg8EuFwDAwNetBVRdXS1JSk5O7nJ9cnJy521fV1hYqEAg0HlJT0/vziUBAPoo58+CKygoUF1dXefl2LFjrpcEAOgF3VpAKSkpkqSampou19fU1HTe9nV+v1+xsbFdLgCAga9bCygjI0MpKSnauXNn53XBYFD79u1TdnZ2d24KANDPGT8LrqGhQWVlZZ0fV1RU6NChQ4qPj9eoUaO0cuVK/fa3v9XVV1+tjIwMPfTQQ0pLS9P8+fO7c90AgH7OuID279+vG264ofPjVatWSZIWLVqkDRs26P7771djY6OWLl2q2tpaXX/99dq+fbvV7CsAwMBlXECzZ8+W53kXvN3n8+mxxx7TY489dlkLgxQRYT4rNioqyjizf/9+44wkJSQkGGdspmLY7AfbFz731i9KF/sZupC6ujrjTGRkpHFGOveZrN+EzdfU0NDQK9sJhULGGUlqb283zgwdOtRqW1ci58+CAwBcmSggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHDCfMwwrNhMJf7yyy+NMzZTjG0mEktSdXW1cWbw4MHGmTNnzhhnbKYYS3aTltva2owzNlO3bY4hn89nnJGk8PBw40xjY6NxxuZ4GDFihHGmpaXFOCPZfW9tREdHG2eampp6YCW9izMgAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCYaS9ZOjQocYZmwGK48aNM8588cUXxhnJbuCnzXBHm+3YDHeUpP/85z/GmbAw89/jbIaE2gzutBmuKkn19fXGGZtj3GYQbigUMs5cc801xhlJ2r59u3HG5nj1+/3GGYaRAgBgiQICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOMIy0l0RGRhpnKioqjDM33nijcaa4uNg4I0mVlZW9sq0f/ehHxpna2lrjjCRFRUUZZ2yGhEZEmP/oNTc3G2cGDRpknJHsBuHGxMQYZ9577z3jzJdffmmcCQ8PN85Idl+TzcBd2+9Tf8cZEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4wTDSXhIKhYwz7e3txhmbAaGrVq0yzkjSnj17jDMZGRnGGZtBrjb7TpKGDh1qnGlqajLO2Aww9fl8xhmb406yW5/NsNRJkyYZZ8rLy40znucZZyS7waI227IdltrfcQYEAHCCAgIAOGFcQLt379ZNN92ktLQ0+Xw+bdmypcvtixcvls/n63KZN29ed60XADBAGBdQY2OjMjMztXbt2gveZ968eaqqquq8bNq06bIWCQAYeIyfhJCXl6e8vLyL3sfv9yslJcV6UQCAga9HHgMqKipSUlKSxo8fr2XLlun06dMXvG9LS4uCwWCXCwBg4Ov2Apo3b56ee+457dy5U//3f/+n4uJi5eXlqaOj47z3LywsVCAQ6Lykp6d395IAAH1Qt78OaOHChZ3/njx5sqZMmaKxY8eqqKhIc+bMOef+BQUFXV6HEgwGKSEAuAL0+NOwx4wZo8TERJWVlZ33dr/fr9jY2C4XAMDA1+MFdPz4cZ0+fVqpqak9vSkAQD9i/Ce4hoaGLmczFRUVOnTokOLj4xUfH69HH31UCxYsUEpKisrLy3X//ffrqquuUm5ubrcuHADQvxkX0P79+3XDDTd0fvzV4zeLFi3SunXrVFpaqr/+9a+qra1VWlqa5s6dq9/85jfy+/3dt2oAQL9nXECzZ8++6LC9f/zjH5e1oIEqIsL8+R42mZKSEuPMU089ZZyRpJqaGuOMzbDPxsZG40xLS4txRpKGDBlinOmtwaI2bIeR9tYA2GHDhhlnbPad7UMANvuhtbXVOBMWdmVORbsyv2oAgHMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA40e1vyY3zGzRokHGmtrbWOGMzQdv2XWhtJgUnJCQYZ2y+pujoaOOMJDU1NfXKtmz2XXh4uHHmYpPru1tHR4dxxmb6eHJysnHmvffeM85IdsdDX/8+9SWcAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAEwwj7SU2wwb9fr9xprKy0jjj8/mMM7a5qKgo48zp06eNM2Fhdr9b2XyfbDI2AytDoZBxxmZAqCS1t7db5XpjOzbDSG33g03OJmMznHYg4AwIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJxgGGkvsRlQaDO4s66uzjhjM+xTksaNG2ecaWlpMc60tbUZZ6Kjo40zkhQRYf4jYTOM1Gb4ZG1trXHGdiirzfps9sOpU6eMM+PHjzfOzJo1yzgjSW+++aZxxuYYv1JxBgQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATjCMtJeEQiHjjM1gTBs2Qy4lu2Gp9fX1xplhw4YZZ1pbW40zkt3gU5uBn+3t7cYZm2OoubnZOCNJgwYNMs7Y7Aeb/V1dXW2cOXnypHHGNjdixAjjjM33diDgDAgA4AQFBABwwqiACgsLNW3aNMXExCgpKUnz58/XkSNHutynublZ+fn5SkhI0NChQ7VgwQLV1NR066IBAP2fUQEVFxcrPz9fe/fu1Y4dO9TW1qa5c+eqsbGx8z733nuvXnvtNb388ssqLi7WiRMndOutt3b7wgEA/ZvRo9zbt2/v8vGGDRuUlJSkAwcOaNasWaqrq9Nf/vIXbdy4UT/4wQ8kSevXr9c111yjvXv36rrrruu+lQMA+rXLegzoq7d/jo+PlyQdOHBAbW1tysnJ6bzPhAkTNGrUKJWUlJz3c7S0tCgYDHa5AAAGPusCCoVCWrlypWbMmKFJkyZJOvv0yKioKMXFxXW5b3Jy8gWfOllYWKhAINB5SU9Pt10SAKAfsS6g/Px8HT58WC+88MJlLaCgoEB1dXWdl2PHjl3W5wMA9A9Wr3Rcvny5tm3bpt27d2vkyJGd16ekpKi1tVW1tbVdzoJqamqUkpJy3s/l9/vl9/ttlgEA6MeMzoA8z9Py5cu1efNm7dq1SxkZGV1unzp1qiIjI7Vz587O644cOaLKykplZ2d3z4oBAAOC0RlQfn6+Nm7cqK1btyomJqbzcZ1AIKDo6GgFAgHdfffdWrVqleLj4xUbG6sVK1YoOzubZ8ABALowKqB169ZJkmbPnt3l+vXr12vx4sWSpD/+8Y8KCwvTggUL1NLSotzcXP35z3/ulsUCAAYOn+d5nutF/K9gMKhAIOB6Gd0uLS3NOOPz+Ywzn3/+uXHmq18sTP30pz81znz44YfGmZaWFuPMmTNnjDOSNHToUONMdHS0ccZm2Of/vuD7m7IZ9ilJHR0dxpnIyEjjjM1A28GDBxtnTp06ZZyRpDvvvNM4Y7MfbL5P/WHCTF1dnWJjYy94O7PgAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4ITVO6LCnM3UX5sJuSNGjDDOvP/++8YZSfr444+NM0uXLjXO2EyBvtA78F7K8OHDjTO1tbXGmWAwaJyxmbodHx9vnJGk5uZm44zNYP2EhATjTGlpqXHmzTffNM5IOudNN7+J48ePW23rSsQZEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4wTDSXtLR0WGcaW9vN84MGzbMOHPq1CnjjCR99NFHxpnCwkLjzOnTp40z4eHhxhlJqqqqMs40NDQYZwYNGmSciYyMNM6EQiHjjCSlpaUZZyIizP87iYmJMc6sX7/eOPPOO+8YZyS74b6VlZXGGZthxQMBZ0AAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4ATDSHvJF198YZyxGUba1NRknAkEAsYZyW4Y6auvvmqcSUxMNM7ExsYaZyS7oZBJSUnGGZthqdHR0caZ+vp644wk/fOf/zTOlJeXG2cOHTpknPnb3/5mnPnxj39snJGklpYW44zNQNuhQ4caZwYCzoAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAmGkfaS3hqoaTNMMzc31zgjSfv27TPOLFy40GpbvSUvL884M2HCBONMc3Ozccbv9xtnbIZ9SlJRUZFVrq+qra21ylVWVhpnIiLM/1u1+Vm3HTTbl3AGBABwggICADhhVECFhYWaNm2aYmJilJSUpPnz5+vIkSNd7jN79mz5fL4ul3vuuadbFw0A6P+MCqi4uFj5+fnau3evduzYoba2Ns2dO1eNjY1d7rdkyRJVVVV1XtasWdOtiwYA9H9Gj5Zt3769y8cbNmxQUlKSDhw4oFmzZnVeP3jwYKWkpHTPCgEAA9JlPQZUV1cnSYqPj+9y/fPPP6/ExERNmjRJBQUFOnPmzAU/R0tLi4LBYJcLAGDgs34adigU0sqVKzVjxgxNmjSp8/o77rhDo0ePVlpamkpLS/XAAw/oyJEjeuWVV877eQoLC/Xoo4/aLgMA0E9ZF1B+fr4OHz6sPXv2dLl+6dKlnf+ePHmyUlNTNWfOHJWXl2vs2LHnfJ6CggKtWrWq8+NgMKj09HTbZQEA+gmrAlq+fLm2bdum3bt3a+TIkRe9b1ZWliSprKzsvAXk9/utXmAHAOjfjArI8zytWLFCmzdvVlFRkTIyMi6Z+eqV2KmpqVYLBAAMTEYFlJ+fr40bN2rr1q2KiYlRdXW1JCkQCCg6Olrl5eXauHGjfvjDHyohIUGlpaW69957NWvWLE2ZMqVHvgAAQP9kVEDr1q2TdPbFpv9r/fr1Wrx4saKiovTWW2/pySefVGNjo9LT07VgwQI9+OCD3bZgAMDAYPwnuItJT09XcXHxZS0IAHBlYBp2L5k5c6ZxxmaCxNdfk/VNvPvuu8aZgeqNN97olQzOspkc3d7ebpxJSEgwzkjSn/70J+OMzdf0r3/9yzhzyy23GGf6GoaRAgCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATPu9SI657WTAYVCAQcL2MPmHSpEnGmWnTphlnDhw4YJyRpNLSUuOMz+ez2lZvCQvrnd/JbH7senPfdXR09Nq2eoPtuy5nZ2cbZ756nzQT5eXlxpm2tjbjTG+rq6tTbGzsBW/nDAgA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADgR4XoBX9fHRtM5ZTOPq7W1tVe2Y6uvf397a319fT8MNLb7u7293Thj8/M0UI+HS31dfW4Y6fHjx5Wenu56GQCAy3Ts2DGNHDnygrf3uQIKhUI6ceKEYmJizpn+GwwGlZ6ermPHjl10wupAx344i/1wFvvhLPbDWX1hP3iep/r6eqWlpV10wnyf+xNcWFjYRRtTkmJjY6/oA+wr7Iez2A9nsR/OYj+c5Xo/fJO31eFJCAAAJyggAIAT/aqA/H6/Vq9ebf3uhgMF++Es9sNZ7Iez2A9n9af90OeehAAAuDL0qzMgAMDAQQEBAJyggAAATlBAAAAn+k0BrV27Vt/61rc0aNAgZWVl6f3333e9pF73yCOPyOfzdblMmDDB9bJ63O7du3XTTTcpLS1NPp9PW7Zs6XK753l6+OGHlZqaqujoaOXk5Ojo0aNuFtuDLrUfFi9efM7xMW/ePDeL7SGFhYWaNm2aYmJilJSUpPnz5+vIkSNd7tPc3Kz8/HwlJCRo6NChWrBggWpqahytuGd8k/0we/bsc46He+65x9GKz69fFNCLL76oVatWafXq1Tp48KAyMzOVm5urkydPul5ar5s4caKqqqo6L3v27HG9pB7X2NiozMxMrV279ry3r1mzRk899ZSeeeYZ7du3T0OGDFFubq6am5t7eaU961L7QZLmzZvX5fjYtGlTL66w5xUXFys/P1979+7Vjh071NbWprlz56qxsbHzPvfee69ee+01vfzyyyouLtaJEyd06623Olx19/sm+0GSlixZ0uV4WLNmjaMVX4DXD0yfPt3Lz8/v/Lijo8NLS0vzCgsLHa6q961evdrLzMx0vQynJHmbN2/u/DgUCnkpKSne448/3nldbW2t5/f7vU2bNjlYYe/4+n7wPM9btGiRd/PNNztZjysnT570JHnFxcWe55393kdGRnovv/xy530++ugjT5JXUlLiapk97uv7wfM87/vf/773i1/8wt2ivoE+fwbU2tqqAwcOKCcnp/O6sLAw5eTkqKSkxOHK3Dh69KjS0tI0ZswY3XnnnaqsrHS9JKcqKipUXV3d5fgIBALKysq6Io+PoqIiJSUlafz48Vq2bJlOnz7tekk9qq6uTpIUHx8vSTpw4IDa2tq6HA8TJkzQqFGjBvTx8PX98JXnn39eiYmJmjRpkgoKCnTmzBkXy7ugPjeM9OtOnTqljo4OJScnd7k+OTlZH3/8saNVuZGVlaUNGzZo/Pjxqqqq0qOPPqqZM2fq8OHDiomJcb08J6qrqyXpvMfHV7ddKebNm6dbb71VGRkZKi8v169//Wvl5eWppKRE4eHhrpfX7UKhkFauXKkZM2Zo0qRJks4eD1FRUYqLi+ty34F8PJxvP0jSHXfcodGjRystLU2lpaV64IEHdOTIEb3yyisOV9tVny8g/FdeXl7nv6dMmaKsrCyNHj1aL730ku6++26HK0NfsHDhws5/T548WVOmTNHYsWNVVFSkOXPmOFxZz8jPz9fhw4eviMdBL+ZC+2Hp0qWd/548ebJSU1M1Z84clZeXa+zYsb29zPPq83+CS0xMVHh4+DnPYqmpqVFKSoqjVfUNcXFxGjdunMrKylwvxZmvjgGOj3ONGTNGiYmJA/L4WL58ubZt26a33367y9u3pKSkqLW1VbW1tV3uP1CPhwvth/PJysqSpD51PPT5AoqKitLUqVO1c+fOzutCoZB27typ7Oxshytzr6GhQeXl5UpNTXW9FGcyMjKUkpLS5fgIBoPat2/fFX98HD9+XKdPnx5Qx4fneVq+fLk2b96sXbt2KSMjo8vtU6dOVWRkZJfj4ciRI6qsrBxQx8Ol9sP5HDp0SJL61vHg+lkQ38QLL7zg+f1+b8OGDd6///1vb+nSpV5cXJxXXV3temm96pe//KVXVFTkVVRUeO+++66Xk5PjJSYmeidPnnS9tB5VX1/vffDBB94HH3zgSfKeeOIJ74MPPvA+++wzz/M87/e//70XFxfnbd261SstLfVuvvlmLyMjw2tqanK88u51sf1QX1/v3XfffV5JSYlXUVHhvfXWW961117rXX311V5zc7PrpXebZcuWeYFAwCsqKvKqqqo6L2fOnOm8zz333OONGjXK27Vrl7d//34vOzvby87Odrjq7nep/VBWVuY99thj3v79+72Kigpv69at3pgxY7xZs2Y5XnlX/aKAPM/znn76aW/UqFFeVFSUN336dG/v3r2ul9TrbrvtNi81NdWLioryRowY4d12221eWVmZ62X1uLffftuTdM5l0aJFnuedfSr2Qw895CUnJ3t+v9+bM2eOd+TIEbeL7gEX2w9nzpzx5s6d6w0fPtyLjIz0Ro8e7S1ZsmTA/ZJ2vq9fkrd+/frO+zQ1NXk///nPvWHDhnmDBw/2brnlFq+qqsrdonvApfZDZWWlN2vWLC8+Pt7z+/3eVVdd5f3qV7/y6urq3C78a3g7BgCAE33+MSAAwMBEAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACf+H4+/+fa5+QEZAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image, label = train_data[0]\n",
        "print(image.shape, label)\n",
        "plt.imshow(image[0], cmap = 'gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFGPmPXTT5i1"
      },
      "source": [
        "2. Recreate the LeNet-5 architecture as your baseline. The model takes as input a greyscale image of size 28x28x1 and has 10 outputs, one for each class. Make sure all parameters (number of neurons, number and size of kernels) is the same as in the original architecture. You may assume that no zero-padding was applied. The model is trained using cross-entropy loss, Adam optimizer with a learning rate of 0.001. Use torch.nn.init.kaiming_uniform to initialize your weights. Use a batch size of 32, unless your hardware doesn't allow you to. Then reduce the size accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BnWpAn0T9XS",
        "outputId": "10a5e100-58b6-4c30-eceb-94a405d5f53f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.init import kaiming_uniform_, zeros_\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Convolutional and pooling layers\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),  # 1 input channel, 6 output channels, 5x5 kernel\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5), # 6 input channels, 16 output channels, 5x5 kernel\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # Fully connected layers\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16 * 4 * 4, 120),  # 16 channels, 4x4 feature map size #NOTE: isn't this 16*5*5?\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, 10),  # 10 output classes\n",
        "        )\n",
        "\n",
        "        self.apply(self.weights_init)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with ReLU activation and max pooling\n",
        "        x = self.conv(x)\n",
        "        # Fully connected layers with ReLU activation\n",
        "        y = self.dense(x)\n",
        "        return y\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight)\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight)\n",
        "            torch.nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------\n",
            "Layer (type)                            Output Shape              Param #\n",
            "==========================================================================\n",
            "Conv2d-1                             [-1, 6, 24, 24]                  156\n",
            "ReLU-2                               [-1, 6, 24, 24]                    0\n",
            "AvgPool2d-3                          [-1, 6, 12, 12]                    0\n",
            "Conv2d-4                              [-1, 16, 8, 8]                2,416\n",
            "ReLU-5                                [-1, 16, 8, 8]                    0\n",
            "AvgPool2d-6                           [-1, 16, 4, 4]                    0\n",
            "Flatten-7                                  [-1, 256]                    0\n",
            "Linear-8                                   [-1, 120]               30,840\n",
            "ReLU-9                                     [-1, 120]                    0\n",
            "Linear-10                                   [-1, 84]               10,164\n",
            "ReLU-11                                     [-1, 84]                    0\n",
            "Linear-12                                   [-1, 10]                  850\n",
            "==========================================================================\n",
            "Total params: 44,426\n",
            "Trainable params: 44,426\n",
            "Non-trainable params: 0\n",
            "--------------------------------------------------------------------------\n",
            "Input size (MB): 0.002991\n",
            "Forward/backward pass size (MB): 0.082047\n",
            "Params size (MB): 0.169472\n",
            "Estimated Total Size (MB): 0.254509\n",
            "--------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchkeras import summary\n",
        "\n",
        "model = LeNet5()\n",
        "\n",
        "def check_weights():\n",
        "    for param in model.parameters():\n",
        "        print(param.data)\n",
        "\n",
        "def summarize(model):\n",
        "    summary(model=model, input_shape=(1,28,28))\n",
        "\n",
        "summarize(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(loader, model, criterion, optimizer):\n",
        "    size = len(loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for n, batch in enumerate(tqdm(loader)):\n",
        "        X,y = batch\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "def test(loader, model, criterion):\n",
        "    size = len(loader.dataset)\n",
        "    batches = len(loader)\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            X,y = batch\n",
        "            pred = model(X)\n",
        "            total_loss += criterion(pred, y).item()\n",
        "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "            \n",
        "    loss = total_loss / batches\n",
        "    accuracy = correct/size\n",
        "    print(f\"loss: {loss}\\naccuracy: {accuracy}\")\n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "----------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [00:22<00:00, 68.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.4752450220215435\n",
            "accuracy: 0.8212\n",
            "--------------\n"
          ]
        }
      ],
      "source": [
        "model = LeNet5()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}\\n----------------\")\n",
        "    train(loader=train_dataloader, model=model, criterion=criterion, optimizer=optimizer)\n",
        "    test(loader=val_dataloader, model=model, criterion=criterion)\n",
        "    print(\"----------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7m3tcJzWwOR"
      },
      "source": [
        "3. Now create four model variants. Each model may differ from the previous model by only one aspect, such that we can compare each pair of subsequent models pair-wise. An aspect should be a meaningful property, e.g., change the type of one layer (convolution --> pooling, etc.), add one layer, use dropout, change your activation function, change the number or size of your kernels, change the learning rate, etc. No use of any merging, attention, recurrent or locally-connected layers. Your variants should be aimed at getting a better performance. We keep the batch sizes fixed so choose a number and keep it constant for all models (including the baseline model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-05fnwZKWx_v"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "options:\n",
        "\n",
        "Dropout\n",
        "Learning rate\n",
        "Activation function\n",
        "More/Less layers\n",
        "Pooling type\n",
        "Normalisation\n",
        "\n",
        "loss_fn/optimizer/weight init (I dont think this will do much)\n",
        "\n",
        "grid-search?\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPl8TpaC799+YwP0pwwQXva",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
