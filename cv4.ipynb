{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evakato/ComputerVision4/blob/main/cv4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIpBlt59Tmmv"
      },
      "source": [
        "1. Import the Fashion MNIST dataset including the data labels. This would import two sets (training set and test set). Create a third set (validation set) by splitting the training set into two (training set and validation set) for validation purposes. Decide what a good ratio of training/validation is, and motivate your choice. You should use the validation set to evaluate the different choices you make when building your CNNs. Keep in mind that the test set will only be used at the very final stage and will not be included in the validation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhn0qdxVSyW0",
        "outputId": "27fd0d63-e3b0-468b-d6d1-40f0ece51573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: Data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "torch.Size([1, 28, 28]) 2\n",
            "Training images shape: (48000, 28, 28)\n",
            "Training labels shape: (48000,)\n",
            "Validation images shape: (12000, 28, 28)\n",
            "Validation labels shape: (12000,)\n",
            "Test images shape: (10000, 28, 28)\n",
            "Test labels shape: (10000,)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = datasets.FashionMNIST('Data', download=True, train=True, transform=ToTensor())\n",
        "test_data = datasets.FashionMNIST('Data', download=True, train=False, transform=ToTensor())\n",
        "\n",
        "print(train_data)\n",
        "splitlength = [50000,10000] #NOTE: This needs to be changed to be calculated using a percentage if we are going to have variable data lenghts (due to e.g. data augmentation)\n",
        "train_data, val_data = random_split(train_data, splitlength)\n",
        "\n",
        "image, label = train_data[0]\n",
        "print(image.shape, label)\n",
        "plt.imshow(image[0], cmap = 'gray')\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size, shuffle=True)\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "val_images = val_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "print(\"Training images shape:\", train_images.shape)\n",
        "print(\"Training labels shape:\", train_labels.shape)\n",
        "print(\"Validation images shape:\", val_images.shape)\n",
        "print(\"Validation labels shape:\", val_labels.shape)\n",
        "print(\"Test images shape:\", test_images.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFGPmPXTT5i1"
      },
      "source": [
        "2. Recreate the LeNet-5 architecture as your baseline. The model takes as input a greyscale image of size 28x28x1 and has 10 outputs, one for each class. Make sure all parameters (number of neurons, number and size of kernels) is the same as in the original architecture. You may assume that no zero-padding was applied. The model is trained using cross-entropy loss, Adam optimizer with a learning rate of 0.001. Use torch.nn.init.kaiming_uniform to initialize your weights. Use a batch size of 32, unless your hardware doesn't allow you to. Then reduce the size accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BnWpAn0T9XS",
        "outputId": "10a5e100-58b6-4c30-eceb-94a405d5f53f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Convolutional and pooling layers\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),  # 1 input channel, 6 output channels, 5x5 kernel\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5), # 6 input channels, 16 output channels, 5x5 kernel\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # Fully connected layers\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16 * 4 * 4, 120),  # 16 channels, 4x4 feature map size #NOTE: isn't this 16*5*5?\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, 10),  # 10 output classes\n",
        "        )\n",
        "\n",
        "        self.apply(self.weights_init)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with ReLU activation and max pooling\n",
        "        x = self.conv(x)\n",
        "        # Fully connected layers with ReLU activation\n",
        "        y = self.dense(x)\n",
        "        return y\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight)\n",
        "            torch.nn.init.zero_(m.bias)\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight)\n",
        "            torch.nn.init.zero_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'torch.nn.init' has no attribute 'zero_'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[63], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLeNet5\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(param\u001b[38;5;241m.\u001b[39mdata)\n",
            "Cell \u001b[1;32mIn[62], line 29\u001b[0m, in \u001b[0;36mLeNet5.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Fully connected layers\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m     21\u001b[0m     nn\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[0;32m     22\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m120\u001b[39m),  \u001b[38;5;66;03m# 16 channels, 4x4 feature map size #NOTE: isn't this 16*5*5?\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m10\u001b[39m),  \u001b[38;5;66;03m# 10 output classes\u001b[39;00m\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_init\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:890\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 890\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:890\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 890\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\yanni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:891\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m    890\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[1;32m--> 891\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "Cell \u001b[1;32mIn[62], line 41\u001b[0m, in \u001b[0;36mLeNet5.weights_init\u001b[1;34m(self, m)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mConv2d):\n\u001b[0;32m     40\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mkaiming_uniform_(m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_\u001b[49m(m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[0;32m     43\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mkaiming_uniform_(m\u001b[38;5;241m.\u001b[39mweight)\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn.init' has no attribute 'zero_'"
          ]
        }
      ],
      "source": [
        "from torchkeras import summary\n",
        "\n",
        "model = LeNet5()\n",
        "for param in model.parameters:\n",
        "    print(param.data)\n",
        "batch_size = 32\n",
        "\n",
        "summary(model=model, input_shape=(1,28,28))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(loader, model, criterion, optimizer):\n",
        "\n",
        "    size = len(loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for n, batch in enumerate(tqdm(loader)):\n",
        "        X,y = batch\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, y.long())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [00:18<00:00, 86.15it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LeNet5()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "for i in range(epochs):\n",
        "    train(loader=train_dataloader, model=model, criterion=criterion, optimizer=optimizer )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7m3tcJzWwOR"
      },
      "source": [
        "3. Now create four model variants. Each model may differ from the previous model by only one aspect, such that we can compare each pair of subsequent models pair-wise. An aspect should be a meaningful property, e.g., change the type of one layer (convolution --> pooling, etc.), add one layer, use dropout, change your activation function, change the number or size of your kernels, change the learning rate, etc. No use of any merging, attention, recurrent or locally-connected layers. Your variants should be aimed at getting a better performance. We keep the batch sizes fixed so choose a number and keep it constant for all models (including the baseline model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-05fnwZKWx_v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPl8TpaC799+YwP0pwwQXva",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
